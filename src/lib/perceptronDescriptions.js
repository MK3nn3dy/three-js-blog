const perceptronDescriptions = [
    "At the time of this post, I'm also following along with the Neural Networks from Scratch book by Harrison Kinsley and Daniel Kukie≈Ça. I thought a basic multi-layered perceptron would be a good project to practice modelling in Cinema 4D and to import and interact with in three js. This project, I focused on camera movements with TWEEN, a Three JS animation library. The descriptions of this model here are very simple. Anyone interested in learning more should definitely check out the Neural Networks from Scratch book and YouTube series.",
    "The input layer of the network is the actual input data we're hoping to classify (in a classification neural network). This array of numbers could represent anything from sensor data, to pixel values of an image.",
    "Hidden layers are collections of individaul neurons. Each neuron recieves the output of every single neuron in the previous layer, whether that's the original input, or the output of a previous hidden layer.",
    "Each neuron has an array of weights corresponding to the array of inputs it receives. It multiplies inputs by the corresponding weight, then adds a 'bias' to each result. There is only one bias per neuron. The weight (a multiplier) adjusts the influence of an input, whereas the bias (an addend) offsets the result. Here, the value of the weights are represented by the brightness of the edges between neurons.",
    "Each neuron passes its output through an activation function, such as this 'rectified linear unit', or 'ReLU', which simply clips the output to 0 if it's negative. If the output is positive, the output is forwarded unchanged. This activation function is crucial as it allows the output of the network to be non-linear in relation to its input.",
    "The output layer is the array of probabilities that our input belongs to each of our classifications. Here, there are only two. The final hidden layer's output goes through a 'softmax' function to remove negative outputs whilst maintaining the relationship between data. This is important for back propogation, the 'tuning' of the network's weights and biases on training data to achieve accurate classifications."
]

export default perceptronDescriptions;